# Big-Data_Challenge

## Background


Many of Amazon's shoppers depend on product reviews to make a purchase. Amazon makes these datasets publicly available. They are quite large and can exceed the capacity of local machines. One dataset alone contains over 1.5 million rows; with over 40 datasets, data analysis can be very demanding on the average local computer. Your first goal for this assignment will be to perform the ETL process completely in the cloud and upload a DataFrame to an RDS instance. The second goal will be to use PySpark or SQL to perform a statistical analysis of selected data.


## Objective


Test ETL skills


## Steps


1. Use the provided schema to create tables in your RDS database.


2. Create two separate Google Colab notebooks and extract any two datasets from the list at review dataset. Put each dataset into its own notebook.


3. Count the number of records (rows) in the dataset.


4. Transform the dataset to fit the tables in the schema file. Making sure that the DataFrames match in data type and in column name.


5. Load the DataFrames that correspond to tables into an RDS instance.


## Tools Used:


1. AWS s3 buckets


2. AWS RDS


3. Google Collab (cloud based jupyter notebook)


4. Pysparks


5. Postgres SQL

